{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 import numpy as np\
import pandas\
\
def normalize_features(array):\
   """\
   Normalize the features in the data set.\
   """\
   array_normalized = (array-array.mean())/array.std()\
   mu = array.mean()\
   sigma = array.std()\
\
   return array_normalized, mu, sigma\
\
def compute_cost(features, values, theta):\
    """\
    Compute the cost function given a set of features / values, \
    and the values for our thetas.\
    \
    This can be the same code as the compute_cost function in the lesson #3 exercises,\
    but feel free to implement your own.\
    """\
    \
    m = len(values)\
    sum_of_square_errors = np.square(np.dot(features, theta) - values).sum()\
    cost = sum_of_square_errors / (2*m)\
\
    return cost\
\
def gradient_descent(features, values, theta, alpha, num_iterations):\
    """\
    Perform gradient descent given a data set with an arbitrary number of features.\
    \
    This can be the same gradient descent code as in the lesson #3 exercises,\
    but feel free to implement your own.\
    """\
    \
    m = len(values)\
    cost_history = []\
\
    for i in range(num_iterations):\
        cost = compute_cost(features, values, theta)\
        cost_history.append(cost)\
        \
        theta = theta + ((alpha / m) * np.dot(values-np.dot(features, theta), features))\
        \
    return theta, pandas.Series(cost_history)\
\
def predictions(dataframe):\
    '''\
    The NYC turnstile data is stored in a pandas dataframe called weather_turnstile.\
    Using the information stored in the dataframe, let's predict the ridership of\
    the NYC subway using linear regression with gradient descent.\
    \
    You can see the information contained in the turnstile weather dataframe here:\
    https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv    \
    \
    Your prediction should have a R^2 value of 0.40 or better.\
    \
    Note: Due to the memory and CPU limitation of our Amazon EC2 instance, we will\
    give you a random subet (~15%) of the data contained in \
    turnstile_data_master_with_weather.csv\
    \
    If you receive a "server has encountered an error" message, that means you are \
    hitting the 30-second limit that's placed on running your program. Try using a \
    smaller number for num_iterations if that's the case.\
    \
    If you are using your own algorithm/models, see if you can optimize your code so \
    that it runs faster.\
    '''\
    \
    dummy_units = pandas.get_dummies(dataframe['UNIT'], prefix='unit')\
    \
    features = dataframe[['rain', 'Hour', 'meantempi', 'precipi']].join(dummy_units)\
    values = dataframe[['ENTRIESn_hourly']]\
    m = len(values)\
    \
    features, mu, sigma = normalize_features(features)\
    \
    features['ones'] = np.ones(m)\
    features_array = np.array(features)\
    values_array = np.array(values).flatten()\
    \
    #Set values for alpha, number of iterations.\
    alpha = 0.1 # please feel free to change this value\
    num_iterations = 75 # please feel free to change this value\
    \
    #Initialize theta, perform gradient descent\
    theta_gradient_descent = np.zeros(len(features.columns))\
    theta_gradient_descent, cost_history = gradient_descent(features_array, \
                                                            values_array, \
                                                            theta_gradient_descent, \
                                                            alpha, \
                                                            num_iterations)\
    \
    predictions = np.dot(features_array, theta_gradient_descent)\
    return predictions\
}